{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T21:36:07.318439Z",
     "iopub.status.busy": "2024-07-19T21:36:07.317441Z",
     "iopub.status.idle": "2024-07-19T21:36:07.344051Z",
     "shell.execute_reply": "2024-07-19T21:36:07.343049Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition data: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get the partition_data parameter from environment variable\n",
    "partition_data = os.environ.get('PARTITION_DATA', 'false').lower() == 'true'\n",
    "\n",
    "print(f\"Partition data: {partition_data}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T21:36:07.370097Z",
     "iopub.status.busy": "2024-07-19T21:36:07.369099Z",
     "iopub.status.idle": "2024-07-19T21:36:13.215703Z",
     "shell.execute_reply": "2024-07-19T21:36:13.213706Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server\n"
     ]
    }
   ],
   "source": [
    "import socketio\n",
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "import openai # Install the OpenAI Python package with `pip install openai`\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Create a SocketIO client\n",
    "sio = socketio.Client()\n",
    "\n",
    "# Connect to the server\n",
    "@sio.event\n",
    "def connect():\n",
    "    print('Connected to server')\n",
    "\n",
    "# Define a function to send a message to the server\n",
    "def send_message_to_server(message):\n",
    "    sio.emit('message_from_notebook', message)\n",
    "# Define a function to send the CSV data to the server\n",
    "def send_csv_data(csv_data):\n",
    "    sio.emit('csv_data', csv_data)\n",
    "# Connect to the server\n",
    "sio.connect('http://localhost:5000')\n",
    "\n",
    "# Send a message to the server\n",
    "#send_message_to_server('Starting finetuning!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T21:36:13.220229Z",
     "iopub.status.busy": "2024-07-19T21:36:13.218720Z",
     "iopub.status.idle": "2024-07-19T21:36:13.231305Z",
     "shell.execute_reply": "2024-07-19T21:36:13.229363Z"
    }
   },
   "outputs": [],
   "source": [
    "# Send a message to the server\n",
    "send_message_to_server('Starting finetuning!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T21:36:13.235292Z",
     "iopub.status.busy": "2024-07-19T21:36:13.235292Z",
     "iopub.status.idle": "2024-07-19T21:36:13.261153Z",
     "shell.execute_reply": "2024-07-19T21:36:13.260152Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all data for fine-tuning...\n",
      "Fine-tuning data saved to finetuning_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "data = pd.read_csv('output/specifications/mega_combined.csv')\n",
    "\n",
    "if partition_data:\n",
    "    print(\"Partitioning data into training and evaluation sets...\")\n",
    "    send_message_to_server('Partitioning data into training and evaluation sets...')\n",
    "    # Partition the data\n",
    "    train_data = data.sample(frac=0.8, random_state=42)\n",
    "    eval_data = data.drop(train_data.index)\n",
    "    \n",
    "    # Save evaluation data for later use\n",
    "    eval_data.to_csv('evaluation_data.csv', index=False)\n",
    "    \n",
    "    # Use train_data for fine-tuning\n",
    "else:\n",
    "    print(\"Using all data for fine-tuning...\")\n",
    "    send_message_to_server('Using all data for fine-tuning...')\n",
    "    # Use all data for fine-tuning\n",
    "    train_data = data\n",
    "\n",
    "# Continue with your fine-tuning process using train_data\n",
    "# Load the CSV file\n",
    "#file_path = 'output/specifications/mega_combined.csv'\n",
    "# Load the CSV file\n",
    "#file_path = os.getenv('PARTITION_FILE', 'output/specifications/mega_combined.csv')\n",
    "#print(f\"Loading CSV file from {file_path}\")\n",
    "df = data\n",
    "\n",
    "# Placeholder for the system message\n",
    "system_message = \"Marv is a factual chatbot that provides complete specifications based on user requirements.\"\n",
    "\n",
    "# Function to create JSONL fine-tuning data\n",
    "def create_finetuning_data(df, system_message):\n",
    "    conversations = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Filter out columns with '-' or blank values\n",
    "        valid_columns = [col for col in df.columns if row[col] not in ['-', '', None]]\n",
    "        \n",
    "        if len(valid_columns) < 1:\n",
    "            continue  # Skip rows where no valid columns are found\n",
    "        \n",
    "        # Randomly select one or two valid columns from the row\n",
    "        selected_columns = random.sample(valid_columns, k=min(2, len(valid_columns)))\n",
    "        \n",
    "        # Construct the user content with the selected columns and their values\n",
    "        user_content = \"Please provide the complete specification for the following requirements: \"\n",
    "        user_content += \", \".join(f\"{col}: {row[col]}\" for col in selected_columns) + \".\"\n",
    "        \n",
    "        # Construct the assistant content with the complete row values\n",
    "        assistant_content = \", \".join(f\"{col}: {row[col]}\" for col in df.columns)\n",
    "        \n",
    "        conversation = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "            ]\n",
    "        }\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Generate the fine-tuning data\n",
    "finetuning_data = create_finetuning_data(df, system_message)\n",
    "\n",
    "# Save the fine-tuning data to a JSONL file\n",
    "output_path = 'finetuning_data.jsonl'\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    for conversation in finetuning_data:\n",
    "        json.dump(conversation, f)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(f\"Fine-tuning data saved to {output_path}\")\n",
    "send_message_to_server('Fine-tuning data saved to {output_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = 'output/specifications/park.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Placeholder for the system message\n",
    "system_message = \"Marv is a factual chatbot that provides complete specifications based on user requirements.\"\n",
    "\n",
    "# Function to create JSONL fine-tuning data\n",
    "def create_finetuning_data(df, system_message):\n",
    "    conversations = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Filter out columns with '-' or blank values\n",
    "        valid_columns = [col for col in df.columns if row[col] not in ['-', '', None]]\n",
    "        \n",
    "        if len(valid_columns) < 1:\n",
    "            continue  # Skip rows where no valid columns are found\n",
    "        \n",
    "        # Randomly select one or two valid columns from the row\n",
    "        selected_columns = random.sample(valid_columns, k=min(2, len(valid_columns)))\n",
    "        \n",
    "        # Construct the user content with the selected columns and their values\n",
    "        user_content = \"Please provide the complete specification for the following requirements: \"\n",
    "        user_content += \", \".join(f\"{col}: {row[col]}\" for col in selected_columns) + \".\"\n",
    "        \n",
    "        # Construct the assistant content with the complete row values\n",
    "        assistant_content = \", \".join(f\"{col}: {row[col]}\" for col in df.columns)\n",
    "        \n",
    "        conversation = {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": system_message},\n",
    "                {\"role\": \"user\", \"content\": user_content},\n",
    "                {\"role\": \"assistant\", \"content\": assistant_content}\n",
    "            ]\n",
    "        }\n",
    "        conversations.append(conversation)\n",
    "    \n",
    "    return conversations\n",
    "\n",
    "# Generate the fine-tuning data\n",
    "finetuning_data = create_finetuning_data(df, system_message)\n",
    "\n",
    "# Print the fine-tuning data instead of saving to a file\n",
    "print(\"Fine-tuning data that would be sent to the model:\")\n",
    "for i, conversation in enumerate(finetuning_data, 1):\n",
    "    print(f\"\\nConversation {i}:\")\n",
    "    print(json.dumps(conversation, indent=2))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(f\"Total number of conversations generated: {len(finetuning_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T21:36:13.266694Z",
     "iopub.status.busy": "2024-07-19T21:36:13.265694Z",
     "iopub.status.idle": "2024-07-19T21:36:14.230276Z",
     "shell.execute_reply": "2024-07-19T21:36:14.229293Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training file ID: file-3koOOtWFxj41vG1q4JTxW5K9\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "file_object = client.files.create(\n",
    "    file=open(\"finetuning_data.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "training_file_id = file_object.id\n",
    "print(f\"Training file ID: {training_file_id}\")\n",
    "send_message_to_server(f\"Created Training file ID: {training_file_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T21:36:14.235810Z",
     "iopub.status.busy": "2024-07-19T21:36:14.234809Z",
     "iopub.status.idle": "2024-07-19T21:36:16.643400Z",
     "shell.execute_reply": "2024-07-19T21:36:16.642453Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftjob-JnHSr27FAmrpmoZ2g4jnmB34\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "fine_tuning_job = client.fine_tuning.jobs.create(\n",
    "    training_file=training_file_id,\n",
    "    model=\"gpt-3.5-turbo\"\n",
    ")\n",
    "print(fine_tuning_job.id)\n",
    "send_message_to_server(f\"Fine-tuning job ID: {fine_tuning_job.id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T21:36:16.648928Z",
     "iopub.status.busy": "2024-07-19T21:36:16.648928Z",
     "iopub.status.idle": "2024-07-19T21:42:53.924831Z",
     "shell.execute_reply": "2024-07-19T21:42:53.922829Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job status: validating_files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job status: validating_files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job status: validating_files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job status: running\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning job completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import time\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# Retrieve the initial job status\n",
    "fine_tuning_job = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)\n",
    "\n",
    "running_status_sent = False\n",
    "\n",
    "while fine_tuning_job.status not in [\"succeeded\", \"cancelled\"]:\n",
    "    if fine_tuning_job.status == \"running\" and not running_status_sent:\n",
    "        print(\"Fine-tuning job status: running\")\n",
    "        send_message_to_server(\"Fine-tuning job status: running\")\n",
    "        running_status_sent = True\n",
    "    elif fine_tuning_job.status != \"running\":\n",
    "        print(f\"Fine-tuning job status: {fine_tuning_job.status}\")\n",
    "        send_message_to_server(f\"Fine-tuning job status: {fine_tuning_job.status}\")\n",
    "        running_status_sent = False  # Reset in case it goes back to running\n",
    "\n",
    "    time.sleep(10)  # Wait for 10 seconds before checking the status again\n",
    "    fine_tuning_job = client.fine_tuning.jobs.retrieve(fine_tuning_job.id)  # Retrieve the latest job status\n",
    "\n",
    "if fine_tuning_job.status == \"cancelled\":\n",
    "    print(\"Fine-tuning job was cancelled!\")\n",
    "    send_message_to_server(\"Fine-tuning job was cancelled!\")\n",
    "elif fine_tuning_job.status == \"succeeded\":\n",
    "    print(\"Fine-tuning job completed successfully!\")\n",
    "    send_message_to_server(\"Fine-tuning job completed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-19T21:42:53.929831Z",
     "iopub.status.busy": "2024-07-19T21:42:53.929831Z",
     "iopub.status.idle": "2024-07-19T21:43:07.158964Z",
     "shell.execute_reply": "2024-07-19T21:43:07.158017Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model ID saved: ft:gpt-3.5-turbo-0125:personal::9mpfBKCV\n",
      "CSV headers saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is restarting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for server to restart...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is back online\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "def load_or_create_json(filename, default_content):\n",
    "    if not os.path.exists(filename):\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(default_content, f)\n",
    "        print(f\"Created {filename} with default content.\")\n",
    "    \n",
    "    with open(filename, 'r') as f:\n",
    "        content = f.read().strip()\n",
    "        if content:\n",
    "            return json.loads(content)\n",
    "        else:\n",
    "            return default_content\n",
    "\n",
    "def save_model_id_and_restart_server(model_id, headers):\n",
    "    # Load or create model_config.json\n",
    "    model_config = load_or_create_json('model_config.json', {'model_id': \"null\"})\n",
    "    \n",
    "    # Update model_id if it's not \"null\"\n",
    "    if model_id != \"null\":\n",
    "        model_config['model_id'] = model_id\n",
    "    \n",
    "    # Save the updated model config\n",
    "    with open('model_config.json', 'w') as f:\n",
    "        json.dump(model_config, f)\n",
    "    \n",
    "    print(f\"Fine-tuned model ID saved: {model_config['model_id']}\")\n",
    "\n",
    "    # Load or create csv_headers.json\n",
    "    headers_config = load_or_create_json('csv_headers.json', {'headers': []})\n",
    "    \n",
    "    # Update headers\n",
    "    headers_config['headers'] = headers\n",
    "    \n",
    "    # Save the updated headers\n",
    "    with open('csv_headers.json', 'w') as f:\n",
    "        json.dump(headers_config, f)\n",
    "    \n",
    "    print(\"CSV headers saved\")\n",
    "    \n",
    "    # Trigger server restart\n",
    "    try:\n",
    "        requests.post('http://localhost:5000/restart')\n",
    "        print(\"Restart signal sent to server\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Server is restarting...\")\n",
    "    \n",
    "    # Wait for server to come back online\n",
    "    server_up = False\n",
    "    while not server_up:\n",
    "        try:\n",
    "            response = requests.get('http://localhost:5000/healthcheck')\n",
    "            if response.status_code == 200:\n",
    "                server_up = True\n",
    "                print(\"Server is back online\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"Waiting for server to restart...\")\n",
    "            time.sleep(5)\n",
    "\n",
    "# Assuming you have a DataFrame called 'df' with your CSV data\n",
    "headers = df.columns.tolist()\n",
    "\n",
    "# Use this function after fine-tuning\n",
    "if fine_tuning_job.status == \"succeeded\":\n",
    "    fine_tuned_model_id = fine_tuning_job.fine_tuned_model\n",
    "    save_model_id_and_restart_server(fine_tuned_model_id, headers)\n",
    "else:\n",
    "    print(\"Fine-tuning job did not succeed. Model ID not saved.\")\n",
    "    send_message_to_server(\"Fine-tuning job did not succeed. Model ID not saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "def save_model_id(model_id):\n",
    "    with open('model_config.json', 'w') as f:\n",
    "        json.dump({'model_id': model_id}, f)\n",
    "        \n",
    "fine_tuned_model_id = fine_tuning_job.fine_tuned_model\n",
    "save_model_id(fine_tuned_model_id)\n",
    "print(f\"Fine-tuned model ID saved: {fine_tuned_model_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "# Get the fine-tuned model ID\n",
    "fine_tuned_model_id = \"ft:gpt-3.5-turbo-0125:personal::9ZJtzJtN\"\n",
    "\n",
    "# Save it as an environment variable\n",
    "os.environ['FINE_TUNED_MODEL_ID'] = fine_tuned_model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "fine_tuned_model_id = os.environ.get('FINE_TUNED_MODEL_ID')\n",
    "model_to_use = fine_tuned_model_id if fine_tuned_model_id else \"ft:gpt-3.5-turbo-0125:personal::9ZJtzJtN:ckpt-step-72\"\n",
    "print(f\"Using model: {model_to_use}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T08:27:46.417866Z",
     "iopub.status.busy": "2024-06-14T08:27:46.417866Z",
     "iopub.status.idle": "2024-06-14T08:27:48.293281Z",
     "shell.execute_reply": "2024-06-14T08:27:48.293281Z"
    }
   },
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo-0125:personal::9afDwulJ\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"Marv is a factual chatbot that provides complete specifications based on user requirements.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Please provide the complete specification for the following requirements: Method: SD\\({}^{35}\\), Temp.: 120.\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "#print(completion.choices[0].message)\n",
    "message_content = completion.choices[0].message.content\n",
    "print(message_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-14T08:27:48.297282Z",
     "iopub.status.busy": "2024-06-14T08:27:48.297282Z",
     "iopub.status.idle": "2024-06-14T08:27:48.309801Z",
     "shell.execute_reply": "2024-06-14T08:27:48.308800Z"
    }
   },
   "source": [
    "message_content = completion.choices[0].message.content\n",
    "print(message_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
