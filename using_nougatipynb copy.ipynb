{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip3 install --upgrade pip\n",
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117 2\n",
    "#pip install timm==0.5\n",
    "#pip install Timm\n",
    "#pip install transformers==4.38.2 3\n",
    "#pip install nougat-ocr 1\n",
    "#pip install torchvision==0.9.1\n",
    "#\n",
    "from IPython import display\n",
    "#nougat pdf 'park2016.pdf' --out 'output'\n",
    "#nougat pdf '323.pdf' --out 'output' --recompute --no-skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda create -n myenv python=3.8.19\n",
    "#conda activate new_env\n",
    "#pip install nougat-ocr\n",
    "#pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117\n",
    "#pip install transformers==4.38.2\n",
    "#nougat pdf '323.pdf' --out 'output' --recompute --no-skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.has_mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#MODEL = \"gpt-4o\"\n",
    "MODEL = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core. output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()\n",
    "\n",
    "chain = model | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer the question based on the context below. If you can't \n",
      "answer the question, reply \"I don't know\".\n",
      "\n",
      "Context: Here is some context\n",
      "\n",
      "Question: Here is a question  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}  \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "print(prompt.format(context=\"Here is some context\", question=\"Here is a question\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%pip install astropy[recommended] --upgrade\n",
    "from astropy.table import Table\n",
    "tab = Table.read(r'output/park2016.mmd')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample code for extraction using nougat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "#MODEL = \"gpt-3.5-turbo\"\n",
    "MODEL = \"llama3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'PromptInput',\n",
       " 'type': 'object',\n",
       " 'properties': {'context': {'title': 'Context', 'type': 'string'},\n",
       "  'question': {'title': 'Question', 'type': 'string'}}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.input_schema.schema() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import DocArrayInMemorySearch\n",
    "\n",
    "#vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)\n",
    "# Create the retriever\n",
    "#retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```csv\n",
      "_doctors_,_doctors_,_doctors_,_doctors_\n",
      "Thermopreg(r) 251.78,Tape,0.10,impregnated glass cloth\n",
      "Polyester fece 101.74-07,Tape,0.56,Nonmergrated polyester fece\n",
      "Glasoflex(r) 261.10-03,Tape,0.50,Impregnated glass fece with high resin content\n",
      "Daml(r) 15182/0930,Resin,,Solventless two-component epoxy resin\n",
      "```\n",
      "\n",
      "```csv\n",
      "Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Frontiers,Front\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-4o\"  # or \"llama3\"\n",
    "\n",
    "# Choose the model and embeddings based on the MODEL variable\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "# Initialize the output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}  \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Load and process the MMD file content\n",
    "def load_mmd_content(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Path to your MMD file\n",
    "mmd_file_path = r'output/von-roll-high-voltage.mmd'\n",
    "\n",
    "# Load the MMD content\n",
    "mmd_content = load_mmd_content(mmd_file_path)\n",
    "\n",
    "# Define the chain\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=model,\n",
    "    output_parser=parser\n",
    ")\n",
    "\n",
    "# Create a function to get answers with the MMD content as context\n",
    "def get_answer_with_mmd_context(question, mmd_content):\n",
    "    # Format the prompt with the MMD content as context\n",
    "    formatted_prompt = prompt.format(context=mmd_content, question=question)\n",
    "    \n",
    "    # Invoke the chain with the formatted prompt\n",
    "    answer = chain.invoke({\"context\": mmd_content, \"question\": question})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the function with a sample question\n",
    "question = \"find and extract all the tabular content inside the file to csv format, if you find more than one table give me all and just respond with the csv format straight away with their headers?\"\n",
    "answer = get_answer_with_mmd_context(question, mmd_content)\n",
    "print(answer['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data has been written to output.csv file.\n"
     ]
    }
   ],
   "source": [
    "# Remove leading and trailing triple backticks and 'csv'\n",
    "csv_data = answer['text'].strip('```').replace('csv', '').replace('```', '')\n",
    "\n",
    "# Write the CSV data to a file\n",
    "with open('output/specifications/von-roll-high-voltage.csv', 'w') as file:\n",
    "    file.write(csv_data)\n",
    "\n",
    "print(\"CSV data has been written to output.csv file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned output\\von-roll-high-voltage.csv\n",
      "CSV cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def clean_csv(file_path):\n",
    "    try:\n",
    "        # Try to read the CSV file with utf-8 encoding\n",
    "        df = pd.read_csv(file_path, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # If there's a UnicodeDecodeError, try with a different encoding\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='latin1')\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Warning: {file_path} is empty or unreadable.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Drop empty rows\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    \n",
    "    try:\n",
    "        # Write the cleaned data back to the CSV file\n",
    "        df.to_csv(file_path, index=False, encoding='utf-8')\n",
    "        print(f\"Cleaned {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {file_path}: {e}\")\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder_path = 'output'\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for file_name in os.listdir(csv_folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_folder_path, file_name)\n",
    "        clean_csv(file_path)\n",
    "\n",
    "print(\"CSV cleaning complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_models import ChatOllama, ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-4\"  # or \"llama2\"\n",
    "\n",
    "# Initialize the model and embeddings\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = ChatOllama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}  \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Function to load CSV content\n",
    "def load_csv_content(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.to_csv(index=False)\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder_path = 'output'\n",
    "output_folder_path = 'output'\n",
    "specifications_folder_path = os.path.join(output_folder_path, 'specifications')\n",
    "\n",
    "# Create the specifications folder if it doesn't exist\n",
    "os.makedirs(specifications_folder_path, exist_ok=True)\n",
    "\n",
    "# Pipeline the prompt with the model and parser\n",
    "pipeline = prompt | model | StrOutputParser()\n",
    "\n",
    "# Create a function to get answers with the CSV content as context\n",
    "def get_answer_with_csv_context(question, csv_content):\n",
    "    # Invoke the pipeline with the context and question\n",
    "    answer = pipeline.invoke({\"context\": csv_content, \"question\": question})\n",
    "    return answer\n",
    "\n",
    "# Define the question to check for specifications\n",
    "question = \"Does the CSV data contain specifications for conductive film producion in R2R?\"\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for file_name in os.listdir(csv_folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_folder_path, file_name)\n",
    "        csv_content = load_csv_content(file_path)\n",
    "        \n",
    "        # Get the answer from the LLM\n",
    "        answer = get_answer_with_csv_context(question, csv_content)\n",
    "        \n",
    "        # If the answer is \"Yes\", move the file to the specifications folder\n",
    "        if 'yes' in answer.lower():\n",
    "            shutil.move(file_path, os.path.join(specifications_folder_path, file_name))\n",
    "\n",
    "print(\"Processing complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined CSV saved to output/specifications/combined.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Function to standardize and combine CSV files\n",
    "def combine_csv_files(folder_path, output_file):\n",
    "    # Collect all unique headers\n",
    "    headers = set()\n",
    "    csv_files = [file for file in os.listdir(folder_path) if file.endswith('.csv')]\n",
    "\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "        df = pd.read_csv(file_path, encoding='utf-8', engine='python')\n",
    "        headers.update(df.columns)\n",
    "\n",
    "    headers = list(headers)\n",
    "    \n",
    "    combined_data = []\n",
    "\n",
    "    # Standardize each CSV file and combine data\n",
    "    for csv_file in csv_files:\n",
    "        file_path = os.path.join(folder_path, csv_file)\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding='utf-8', engine='python')\n",
    "        except pd.errors.ParserError:\n",
    "            print(f\"Skipped file {csv_file} due to parsing error.\")\n",
    "            continue\n",
    "        \n",
    "        standardized_data = OrderedDict((header, []) for header in headers)\n",
    "        \n",
    "        for header in headers:\n",
    "            if header in df.columns:\n",
    "                standardized_data[header] = df[header].tolist()\n",
    "            else:\n",
    "                standardized_data[header] = [None] * len(df)\n",
    "        \n",
    "        combined_df = pd.DataFrame(standardized_data)\n",
    "        combined_data.append(combined_df)\n",
    "\n",
    "    # Concatenate all standardized data into a single DataFrame\n",
    "    final_df = pd.concat(combined_data, ignore_index=True)\n",
    "\n",
    "    # Save the combined DataFrame to a CSV file\n",
    "    final_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "\n",
    "    print(f\"Combined CSV saved to {output_file}\")\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder_path = 'output/specifications'\n",
    "output_csv_path = 'output/specifications/combined.csv'\n",
    "\n",
    "# Combine the CSV files\n",
    "combine_csv_files(csv_folder_path, output_csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def extract_tabular_content(latex_content):\n",
    "    # Regular expression pattern to match tabular environments\n",
    "    tabular_pattern = re.compile(r'\\\\begin{tabular}[^\\\\]*\\\\\\\\hline\\\\s*\\\\\\\\hline(.*?)\\\\\\\\hline\\\\s*\\\\\\\\end{tabular}', re.DOTALL)\n",
    "    # Find all matches of tabular environments in the LaTeX content\n",
    "    tabular_matches = tabular_pattern.findall(latex_content)\n",
    "    return tabular_matches\n",
    "\n",
    "def extract_nested_tabulars(tabular_matches):\n",
    "    nested_tabulars = []\n",
    "    # Iterate through tabular matches\n",
    "    for tabular in tabular_matches:\n",
    "        # Initialize a stack to keep track of nested environments\n",
    "        stack = []\n",
    "        nested_tabular = \"\"\n",
    "        # Iterate through each character in the tabular content\n",
    "        for char in tabular:\n",
    "            if char == '\\\\':\n",
    "                # Check if a LaTeX command is encountered\n",
    "                stack.append('\\\\')\n",
    "            elif char == '{':\n",
    "                # Check if an opening brace is encountered\n",
    "                stack.append('{')\n",
    "            elif char == '}':\n",
    "                # Check if a closing brace is encountered\n",
    "                if stack and stack[-1] == '{':\n",
    "                    stack.pop()\n",
    "            elif char == 'e' and stack and stack[-1] == '\\\\':  # Check if stack is not empty\n",
    "                # Check if the command \"\\\\end\" is encountered\n",
    "                stack.pop() # Remove \"\\\\\"\n",
    "                if stack and stack[-1] == '\\\\':  # Check if stack is not empty\n",
    "                    # If the previous character was \"\\\\\", it's the beginning of \"\\\\end\"\n",
    "                    stack.pop() # Remove \"\\\\\"\n",
    "                    if len(stack) >= 5 and ''.join(stack[-5:]) == \"tabular\":\n",
    "                        # If the last 7 characters are \"tabular\", it's the end of the tabular environment\n",
    "                        nested_tabular += char\n",
    "                        nested_tabulars.append(nested_tabular.strip())\n",
    "                        break\n",
    "            nested_tabular += char\n",
    "    return nested_tabulars\n",
    "\n",
    "def main(input_file):\n",
    "    # Read the LaTeX content from the input file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        latex_content = file.read()\n",
    "    print(latex_content)  # Add this line to print the input file content\n",
    "    # ...\n",
    "    # Extract tabular content from the LaTeX content\n",
    "    tabular_matches = extract_tabular_content(latex_content)\n",
    "    print(f\"Number of tabular matches: {len(tabular_matches)}\")  # Add this line\n",
    "    # Extract nested tabulars from tabular matches\n",
    "    nested_tabulars = extract_nested_tabulars(tabular_matches)\n",
    "    print(f\"Number of nested tabulars: {len(nested_tabulars)}\")  # Add this line\n",
    "    # Print the extracted tabular content\n",
    "    for idx, tabular in enumerate(nested_tabulars, start=1):\n",
    "        print(f\"Tabular {idx}:\\n{tabular}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r'output/park2016.mmd'\n",
    "    main(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import re\n",
    "\n",
    "def extract_nested_environments(latex_content, environment):\n",
    "    pattern = re.compile(r'\\\\begin{' + environment + r'}(.*?)\\\\end{' + environment + r'}', re.DOTALL)\n",
    "    matches = []\n",
    "    stack = []\n",
    "    start_pos = 0\n",
    "\n",
    "    for match in re.finditer(pattern, latex_content):\n",
    "        if '\\\\begin{' + environment + '}' in match.group(0):\n",
    "            if not stack:\n",
    "                start_pos = match.start()\n",
    "            stack.append('begin')\n",
    "        if '\\\\end{' + environment + '}' in match.group(0):\n",
    "            stack.pop()\n",
    "            if not stack:\n",
    "                matches.append(latex_content[start_pos:match.end()])\n",
    "    \n",
    "    return matches\n",
    "\n",
    "def extract_tabular_from_table(table_content):\n",
    "    tabular_pattern = re.compile(r'\\\\begin{tabular}{.*?}(.*?)\\\\end{tabular}', re.DOTALL)\n",
    "    return tabular_pattern.findall(table_content)\n",
    "\n",
    "def convert_latex_to_csv(latex_table):\n",
    "    # Clean up LaTeX commands and convert to CSV format\n",
    "    latex_table = re.sub(r'\\\\[a-zA-Z]+\\{[^}]*\\}', '', latex_table)\n",
    "    latex_table = re.sub(r'\\\\[a-zA-Z]+', '', latex_table)\n",
    "    latex_table = re.sub(r'\\{[^}]*\\}', '', latex_table)\n",
    "    latex_table = re.sub(r'\\s+', ' ', latex_table)\n",
    "    latex_table = latex_table.replace('&', ',').replace('\\\\\\\\', '\\n').strip()\n",
    "    latex_table = re.sub(r'\\s*\\\\hline\\s*', '', latex_table)\n",
    "    return latex_table\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "def write_file(file_path, content):\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def main(input_file, output_file):\n",
    "    latex_content = read_file(input_file)\n",
    "    table_blocks = extract_nested_environments(latex_content, 'table')\n",
    "    \n",
    "    if table_blocks:\n",
    "        csv_tables = []\n",
    "        for table_block in table_blocks:\n",
    "            tabular_contents = extract_tabular_from_table(table_block)\n",
    "            for tabular_content in tabular_contents:\n",
    "                csv_table = convert_latex_to_csv(tabular_content)\n",
    "                csv_tables.append(csv_table)\n",
    "        \n",
    "        all_csv_tables = '\\n\\n'.join(csv_tables)\n",
    "        write_file(output_file, all_csv_tables)\n",
    "        print(f\"Extracted tables have been written to {output_file}\")\n",
    "    else:\n",
    "        print(\"No tables found in the input file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = r'output/park2016.mmd'  # Replace with your input .mmd file path\n",
    "    output_file = r'output.csv'  # Replace with your desired output CSV file path\n",
    "    main(input_file, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
