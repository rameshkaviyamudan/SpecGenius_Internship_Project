{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T13:09:04.060711Z",
     "iopub.status.busy": "2024-08-02T13:09:04.059680Z",
     "iopub.status.idle": "2024-08-02T13:09:08.596845Z",
     "shell.execute_reply": "2024-08-02T13:09:08.595836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to server\n"
     ]
    }
   ],
   "source": [
    "import socketio\n",
    "\n",
    "# Create a SocketIO client\n",
    "sio = socketio.Client()\n",
    "\n",
    "# Connect to the server\n",
    "@sio.event\n",
    "def connect():\n",
    "    print('Connected to server')\n",
    "\n",
    "# Define a function to send a message to the server\n",
    "def send_message_to_server(message):\n",
    "    sio.emit('message_from_notebook', message)\n",
    "# Define a function to send the CSV data to the server\n",
    "def send_csv_data(csv_data):\n",
    "    sio.emit('csv_data', csv_data)\n",
    "# Connect to the server\n",
    "sio.connect('http://localhost:5000')\n",
    "\n",
    "# Send a message to the server\n",
    "send_message_to_server('Notebook Connected!')\n",
    "\n",
    "#csv_data = '\"Name\",\"Age\",\"City\",\"Country\",\"Job\",\"Salary\",\"Email\",\"Phone\",\"Birthday\",\"Height\",\"Weight\"'\n",
    "#send_csv_data(csv_data)\n",
    "\n",
    "#send_csv_data(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T13:09:08.645840Z",
     "iopub.status.busy": "2024-08-02T13:09:08.644838Z",
     "iopub.status.idle": "2024-08-02T13:09:37.859442Z",
     "shell.execute_reply": "2024-08-02T13:09:37.858445Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading MMD content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\windows\\system32\\project\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing content\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from flask import flash\n",
    "from time import sleep\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-4o\"  # or \"llama3\"\n",
    "\n",
    "# Choose the model and embeddings based on the MODEL variable\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = Ollama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "# Initialize the output parser\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. If you can't \n",
    "answer the question, reply \"I don't know\".\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}  \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "# Load and process the MMD file content\n",
    "def load_mmd_content(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    return content\n",
    "\n",
    "# Test the function with a sample question\n",
    "question = \"find and extract all the tabular content inside the file to csv format, if you find more than one table give me all and just respond with the csv format straight away with their headers?\"\n",
    "\n",
    "# Get paths from environment variables\n",
    "UPLOAD_FOLDER = os.environ.get('UPLOAD_FOLDER', 'uploads')\n",
    "PROCESSED_FOLDER = os.environ.get('PROCESSED_FOLDER', 'processed')\n",
    "OUTPUT_FOLDER = os.environ.get('OUTPUT_FOLDER', 'uploads/output')\n",
    "NOUGAT_PDF_FILENAME = os.environ.get('NOUGAT_PDF_FILENAME', '')\n",
    "\n",
    "# Get the filename from the environment variable\n",
    "filename = os.environ.get('NOUGAT_PDF_FILENAME', '')\n",
    "#filename = 'park.pdf'\n",
    "\n",
    "# Construct the path to the MMD file\n",
    "# Construct the path to the MMD file\n",
    "mmd_file_path = os.path.join(OUTPUT_FOLDER, filename.replace('.pdf', '.mmd'))\n",
    "#mmd_file_path = 'uploads/output/park.mmd'\n",
    "\n",
    "# Process the content\n",
    "print(\"Loading MMD content\")\n",
    "#emit('progress', {'message': 'Loading MMD content'})\n",
    "\n",
    "# Load the MMD content\n",
    "mmd_content = load_mmd_content(mmd_file_path)\n",
    "\n",
    "# Define the chain\n",
    "chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=model,\n",
    "    output_parser=parser\n",
    ")\n",
    "\n",
    "# Create a function to get answers with the MMD content as context\n",
    "def get_answer_with_mmd_context(question, mmd_content):\n",
    "    # Format the prompt with the MMD content as context\n",
    "    formatted_prompt = prompt.format(context=mmd_content, question=question)\n",
    "    \n",
    "    # Invoke the chain with the formatted prompt\n",
    "    answer = chain.invoke({\"context\": mmd_content, \"question\": question})\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Test the function with a sample question\n",
    "answer = get_answer_with_mmd_context(question, mmd_content)\n",
    "#print(answer['text'])  # Print the answer text in terminal\n",
    "\n",
    "# Check if the answer is \"I don't know\"\n",
    "if answer['text'] == \"I don't know\":\n",
    "    print(\"Notebook finished: No tables found\")  # Indicate failure to Flask app\n",
    "    send_message_to_server(f\"Processed PDF, No tables found in {NOUGAT_PDF_FILENAME}\")\n",
    "\n",
    "\n",
    "\n",
    "send_message_to_server(f\"Found and extracted tables from {NOUGAT_PDF_FILENAME}\")\n",
    "\n",
    "# Process the content\n",
    "print(\"Processing content\")\n",
    "#emit_progress(\"Notebook execution started.\")\n",
    "#emit('progress', {'message': ' lolzz '})\n",
    "\n",
    "\n",
    "# Print environment variables\n",
    "#send_message_to_server(f\"UPLOAD_FOLDER: {UPLOAD_FOLDER}\")\n",
    "#send_message_to_server(f\"PROCESSED_FOLDER: {PROCESSED_FOLDER}\")\n",
    "#send_message_to_server(f\"OUTPUT_FOLDER: {OUTPUT_FOLDER}\")\n",
    "#send_message_to_server(f\"NOUGAT_PDF_FILENAME: {NOUGAT_PDF_FILENAME}\")\n",
    "# Send a message to the server\n",
    "#send_message_to_server('Notebook Connected!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T13:09:37.864449Z",
     "iopub.status.busy": "2024-08-02T13:09:37.863448Z",
     "iopub.status.idle": "2024-08-02T13:09:37.876442Z",
     "shell.execute_reply": "2024-08-02T13:09:37.874442Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV data has been written to output\\park.csv file.\n",
      "\n",
      "Method,App.,Material,Solid,Âµ,V,h0,fr,Temp.,w,tvaridity\n",
      "D(21),DSSC,TiO2,10wt%,-,1,-,-,120-140,-,10 (wet)\n",
      "DG(22),**-**,**-**,992-1088 kg/m3,1-4,19.8-210,-,-,-,100,1-10 (dry)\n",
      "K(28),e-paper,TiO2,-,-,9.14,-,-,179-192,**-**,12-40 (dry)\n",
      "K(37),OPV,ZnO,50 mg/ml,-,0.8,300,-,125-130,250,0.02 (dry)\n",
      "K/MG(33),Battery (Li),Carbon,-,-,0.25-10,25 (K),-,875-1185,2-200 (dry)\n",
      "K/SD(34),Fuel cell,PBI,9wt%,-,0.20,400,-,140,250,40 (dry)\n",
      "LFS(24),Paper,TiO2,50 mg/ml,-,150,10^3-25-10^4,23,75-110,50,1 (dry)\n",
      "MG(32),Sensor (flex),Carbon,1120 kg/m3,20,0.3 (web),-,-,130,150,4.31 (dry)\n",
      "MG(31),TCF,PEDOT:PSS,-,1-1000,0.18,-,-,-,-,0.02 (dry)\n",
      "SD(6),AR,Si,1.5-5,3.3,1.6-4.4,100,2-13,140,100,0.06-0.13 (dry)\n",
      "SD(73),OPV,P3HT:PCBM,13.5 mg/ml,-,2,-,1.7,140,<305,0.129 (dry)\n",
      "SD(88),OPV,,60 mg/ml,25,240,-,-,140,**-**,100 (wet)\n",
      "SD(33),,Ag,-,-,1.2-2.4,100,2-7,**-**,250,0.67-0.252 (dry)\n",
      "SD(35),PSA,PSA,20wt%,1800,2-10,100-400,10.65-53.74,120,150,4.5-5 (dry)\n",
      "SD(34),TCF,,0.5wt%,**-**,2.7,100,3-25,150,260,**-**\n",
      "SD(35),TCF,,10-20wt%,,,,,,,\n",
      "SD(36),TCF,,-,2.12-8.68,1-2,30-50,3-9,110,250,1-26 (wet)\n",
      "S(35),SC,CNT,1 mg/ml,-,4 cm2/s,-,-,-,-,4 (dry)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construct the CSV file path dynamically using the filename variable\n",
    "csv_file_path = os.path.join('output', f'{filename.replace(\".pdf\", \".csv\")}')\n",
    "#csv_file_path = 'output/park.csv'\n",
    "# Check if the answer is \"I don't know\"\n",
    "if answer['text'] == \"I don't know\":\n",
    "    print(\"No tables found\")\n",
    "    send_message_to_server('No tables found')\n",
    "    #raise ValueError(\"No tables found\")\n",
    "# Remove leading and trailing triple backticks and 'csv'\n",
    "csv_data = answer['text'].strip('```').replace('csv', '').replace('```', '')\n",
    "#csv_data = answer['text']\n",
    "send_csv_data(csv_data)\n",
    "\n",
    "# Continue with the subsequent code cells\n",
    "# Write the CSV data to the dynamically constructed file path\n",
    "with open(csv_file_path, 'w', encoding='utf-8') as file:\n",
    "    file.write(csv_data)\n",
    "#print(answer['text'])\n",
    "# Instead of print(answer['text']), use:\n",
    "print(f\"CSV data has been written to {csv_file_path} file.\")\n",
    "print(csv_data)\n",
    "send_message_to_server(f\"CSV data has been written to {csv_file_path} file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T13:09:37.882442Z",
     "iopub.status.busy": "2024-08-02T13:09:37.881444Z",
     "iopub.status.idle": "2024-08-02T13:09:38.864494Z",
     "shell.execute_reply": "2024-08-02T13:09:38.863510Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to output\\park.csv\n",
      "CSV cleaning complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Function to clean CSV file\n",
    "def clean_cell(cell):\n",
    "    if isinstance(cell, str):\n",
    "        # Remove superscripts and subscripts like in DG\\({}^{22}\\) and TiO\\({}_{2}\\)\n",
    "        cell = re.sub(r'\\\\?\\({}(\\^)\\{[^}]*\\}\\)', '', cell)\n",
    "        # Remove any remaining parentheses and their contents\n",
    "        cell = re.sub(r'\\([^)]*\\)', '', cell)\n",
    "        # Remove any leftover backslashes\n",
    "        cell = cell.replace('\\\\', '')\n",
    "        return cell.strip()\n",
    "    return cell\n",
    "\n",
    "def clean_csv(input_file):\n",
    "    try:\n",
    "        # Try to read the CSV file with utf-8 encoding\n",
    "        df = pd.read_csv(input_file, encoding='utf-8')\n",
    "    except UnicodeDecodeError:\n",
    "        # If there's a UnicodeDecodeError, try with a different encoding\n",
    "        try:\n",
    "            df = pd.read_csv(input_file, encoding='latin1')\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {input_file}: {e}\")\n",
    "            return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"Warning: {input_file} is empty or unreadable.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {input_file}: {e}\")\n",
    "        return\n",
    "\n",
    "    # Drop empty rows\n",
    "    df.dropna(how='all', inplace=True)\n",
    "    \n",
    "    # Check if 'Method' column exists\n",
    "    if 'Method' in df.columns:\n",
    "        # Apply the cleaning function only to the 'Method' column\n",
    "        df['Method'] = df['Method'].apply(clean_cell)\n",
    "\n",
    "    try:\n",
    "        # Write the cleaned data to the new CSV file\n",
    "        df.to_csv(input_file, index=False, encoding='utf-8')\n",
    "        print(f\"Cleaned data saved to {input_file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {input_file}: {e}\")\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder_path = 'output'\n",
    "output_folder_path = 'output'\n",
    "\n",
    "# Get the NOUGAT_PDF_FILENAME from the environment variable\n",
    "nougat_pdf_filename = os.environ.get('NOUGAT_PDF_FILENAME', '')\n",
    "#nougat_pdf_filename = 'park.pdf'\n",
    "\n",
    "# Construct the new CSV file path by stripping the .pdf extension and adding .csv\n",
    "new_csv_path = os.path.join(csv_folder_path, os.path.splitext(nougat_pdf_filename)[0] + '.csv')\n",
    "clean_csv(new_csv_path)\n",
    "\n",
    "print(\"CSV cleaning complete.\")\n",
    "#send_message_to_server(f\"Cleaned the uploaded CSV file: {new_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T13:09:38.870510Z",
     "iopub.status.busy": "2024-08-02T13:09:38.869509Z",
     "iopub.status.idle": "2024-08-02T13:09:40.148050Z",
     "shell.execute_reply": "2024-08-02T13:09:40.147076Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\windows\\system32\\project\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import ChatOpenAI`.\n",
      "  warn_deprecated(\n",
      "c:\\windows\\system32\\project\\venv\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 0.3.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes\n",
      "Yes\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from langchain_community.chat_models import ChatOllama, ChatOpenAI\n",
    "from langchain_community.embeddings import OllamaEmbeddings, OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-4o\" #\"llama2\"  # or \"gpt-4\"\n",
    "\n",
    "# Initialize the model and embeddings\n",
    "if MODEL.startswith(\"gpt\"):\n",
    "    model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "else:\n",
    "    model = ChatOllama(model=MODEL)\n",
    "    embeddings = OllamaEmbeddings(model=MODEL)\n",
    "\n",
    "# Define the prompt template\n",
    "template = \"\"\"\n",
    "Answer the question based on the context below. \n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Question: {question}  \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"], \n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Function to load CSV content\n",
    "def load_csv_content(file_path):\n",
    "    encodings = ['utf-8', 'latin1', 'iso-8859-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            return df.to_csv(index=False)\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Error decoding {file_path} with {encoding}, trying next encoding.\")\n",
    "    raise ValueError(f\"Unable to decode file {file_path} with available encodings.\")\n",
    "\n",
    "# Path to the folder containing CSV files\n",
    "csv_folder_path = 'output'\n",
    "output_folder_path = 'output'\n",
    "specifications_folder_path = os.path.join(output_folder_path, 'specifications')\n",
    "\n",
    "# Create the specifications folder if it doesn't exist\n",
    "os.makedirs(specifications_folder_path, exist_ok=True)\n",
    "\n",
    "# Pipeline the prompt with the model and parser\n",
    "pipeline = prompt | model | StrOutputParser()\n",
    "\n",
    "# Create a function to get answers with the CSV content as context\n",
    "def get_answer_with_csv_context(question, csv_content):\n",
    "    # Invoke the pipeline with the context and question\n",
    "    answer = pipeline.invoke({\"context\": csv_content, \"question\": question})\n",
    "    print(answer)\n",
    "    return answer\n",
    "\n",
    "# Define the question to check for specifications\n",
    "question = \"Does the CSV data contain specifications, just reply with yes or no?\"\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for file_name in os.listdir(csv_folder_path):\n",
    "    if file_name.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_folder_path, file_name)\n",
    "        try:\n",
    "            csv_content = load_csv_content(file_path)\n",
    "            \n",
    "            # Get the answer from the LLM\n",
    "            answer = get_answer_with_csv_context(question, csv_content)\n",
    "            print(answer)  # Remove the parentheses here\n",
    "            # If the answer is \"Yes\", move the file to the specifications folder\n",
    "            if 'yes' in answer.lower():\n",
    "                shutil.move(file_path, os.path.join(specifications_folder_path, file_name))\n",
    "            else:\n",
    "                print(\"No specifications found in\", file_name)\n",
    "                send_message_to_server(f\"No specifications found in {file_name}\")\n",
    "\n",
    "        except ValueError as e:\n",
    "            print(e)\n",
    "#send_csv_data(csv_data)\n",
    "\n",
    "print(\"Processing complete.\")\n",
    "send_message_to_server(f\"Idenified and moved files with specifications to {specifications_folder_path} folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T13:09:40.153044Z",
     "iopub.status.busy": "2024-08-02T13:09:40.153044Z",
     "iopub.status.idle": "2024-08-02T13:09:49.882376Z",
     "shell.execute_reply": "2024-08-02T13:09:49.881379Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated combined CSV data saved to output/specifications\\mega_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "import pandas as pd\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "MODEL = \"gpt-3.5-turbo\"  # or another OpenAI model\n",
    "OUTPUT_FOLDER = 'output'\n",
    "OUTPUT_SPECS_FOLDER = 'output/specifications'\n",
    "\n",
    "# Initialize the model and embeddings\n",
    "model = ChatOpenAI(openai_api_key=OPENAI_API_KEY, model=MODEL)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Define the prompt template for understanding and combining CSV data\n",
    "template = \"\"\"\\\n",
    "You are an expert in data analysis. I have provided you with the contents of the combined CSV file and a new CSV file. Your task is to:\n",
    "\n",
    "1. Understand the data in the combined CSV file and the new CSV file thoroughly.\n",
    "2. Identify similar or related data between these two files.\n",
    "3. Combine and consolidate the data from the new CSV file into the combined dataset.\n",
    "4. Provide the updated combined data in CSV format, with appropriate headers.\n",
    "\n",
    "Here are the contents of the combined CSV file:\n",
    "\n",
    "{combined_csv_contents}\n",
    "\n",
    "And here are the contents of the new CSV file:\n",
    "\n",
    "{new_csv_contents}\n",
    "\n",
    "Now, analyze these files, combine the new data into the combined dataset, if ou find data not compatible with each other then have different headers for both and provide the updated result in CSV format straight away starting from '''\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(input_variables=[\"combined_csv_contents\", \"new_csv_contents\"], template=template)\n",
    "\n",
    "def handle_bad_line(line):\n",
    "    print(f\"Skipping bad line: {line}\")\n",
    "\n",
    "def load_csv_content(file_path):\n",
    "    encodings = ['utf-8', 'latin1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as f:\n",
    "                content = f.read().strip()\n",
    "            if not content:\n",
    "                print(f\"The file {file_path} is empty.\")\n",
    "                return \"\"\n",
    "            df = pd.read_csv(file_path, encoding=encoding, engine='python', on_bad_lines='skip')\n",
    "            return df.to_csv(index=False)\n",
    "        except (UnicodeDecodeError, pd.errors.ParserError):\n",
    "            print(f\"Error decoding {file_path} with {encoding}, trying next encoding.\")\n",
    "    raise ValueError(f\"Unable to decode file {file_path} with available encodings.\")\n",
    "\n",
    "# Get the NOUGAT_PDF_FILENAME from the environment variable\n",
    "nougat_pdf_filename = os.environ.get('NOUGAT_PDF_FILENAME', '')\n",
    "#nougat_pdf_filename = 'park.pdf'\n",
    "\n",
    "# Construct the new CSV file path by stripping the .pdf extension and adding .csv\n",
    "new_csv_path = os.path.join(OUTPUT_FOLDER, os.path.splitext(nougat_pdf_filename)[0] + '.csv')\n",
    "#new_csv_path = 'output/specifications/park.csv'\n",
    "if os.path.isfile(new_csv_path):\n",
    "    new_csv_contents = load_csv_content(new_csv_path)\n",
    "else:\n",
    "    new_csv_path = os.path.join(OUTPUT_SPECS_FOLDER, os.path.splitext(nougat_pdf_filename)[0] + '.csv')\n",
    "    if os.path.isfile(new_csv_path):\n",
    "        new_csv_contents = load_csv_content(new_csv_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find the CSV file for '{nougat_pdf_filename}' in either '{OUTPUT_FOLDER}' or '{OUTPUT_SPECS_FOLDER}'\")\n",
    "\n",
    "# Load the combined CSV file or create a new one\n",
    "combined_csv_path = os.path.join(OUTPUT_SPECS_FOLDER, \"mega_combined.csv\")\n",
    "if os.path.isfile(combined_csv_path):\n",
    "    combined_csv_contents = load_csv_content(combined_csv_path)\n",
    "    if not combined_csv_contents:\n",
    "        print(\"The combined CSV file is empty. Using the new CSV contents.\")\n",
    "        combined_csv_contents = new_csv_contents\n",
    "else:\n",
    "    combined_csv_contents = new_csv_contents\n",
    "\n",
    "# If the combined CSV file doesn't exist or is empty, create it with the new CSV contents\n",
    "if not os.path.isfile(combined_csv_path) or not combined_csv_contents:\n",
    "    with open(combined_csv_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(new_csv_contents)\n",
    "    print(f\"Created new combined CSV file at {combined_csv_path} with the contents of {new_csv_path}\")\n",
    "    send_message_to_server(f\"Created new combined CSV file at {combined_csv_path} with the contents of {new_csv_path}\")\n",
    "    # Exit the script as we've just created the combined file with new contents\n",
    "    exit()\n",
    "\n",
    "# Use GPT to understand and combine the CSV data if the combined CSV file exists\n",
    "if os.path.isfile(combined_csv_path):\n",
    "    pipeline = prompt | model | StrOutputParser()\n",
    "    combined_csv = pipeline.invoke({\"combined_csv_contents\": combined_csv_contents, \"new_csv_contents\": new_csv_contents})\n",
    "    # Strip leading and trailing triple quotes if present\n",
    "    if combined_csv.startswith(\"''''\"):\n",
    "        combined_csv = combined_csv[3:-3].strip()\n",
    "        print(\"stripped triple quotes\")\n",
    "        \n",
    "    # Save the updated combined CSV\n",
    "    updated_csv_path = os.path.join(OUTPUT_SPECS_FOLDER, \"mega_combined.csv\")\n",
    "    with open(updated_csv_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(combined_csv)\n",
    "\n",
    "    print(f\"Updated combined CSV data saved to {updated_csv_path}\")\n",
    "    send_message_to_server(f\"Updated combined CSV data saved to {updated_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-08-02T13:09:49.888378Z",
     "iopub.status.busy": "2024-08-02T13:09:49.887381Z",
     "iopub.status.idle": "2024-08-02T13:10:53.045634Z",
     "shell.execute_reply": "2024-08-02T13:10:53.044664Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is restarting...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for server to restart...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Server is back online\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import time\n",
    "# Trigger server restart\n",
    "# Trigger server restart\n",
    "time.sleep(50)\n",
    "try:\n",
    "    requests.post('http://localhost:5000/restart')\n",
    "    print(\"Restart signal sent to server\")\n",
    "except requests.exceptions.ConnectionError:\n",
    "    print(\"Server is restarting...\")\n",
    "\n",
    "# Wait for server to come back online\n",
    "server_up = False\n",
    "while not server_up:\n",
    "    try:\n",
    "        response = requests.get('http://localhost:5000/healthcheck')\n",
    "        if response.status_code == 200:\n",
    "            server_up = True\n",
    "            print(\"Server is back online\")\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Waiting for server to restart...\")\n",
    "        time.sleep(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
